---
layout:       post
title:        "CLIP"
author:       "Ke"
header-style: text
catalog:      true
mathjax: true
tags:
    - Multi Modality
---

# 动机
在NLP中，预训练的方法目前其实已经被验证很成功了，像BERT和GPT系列之类的。其中，GPT-3从网上搜集了400 billion byte-pair-encoded tokens进行预训练然后可以在很多下游任务上实现SOTA性能和zero-shot learning。这其实说明从web-scale的数据中学习是可以超过高质量的人工标注的NLP数据集的。
然而，对于CV领域，目前预训练模型基本都是基于人工标注的ImageNet数据集（含有1400多万张图像），那么借鉴NLP领域的GPT-3从网上搜集大量数据的思路，我们能不能也从网上搜集大量图像数据用于训练视觉表征模型呢？
# 启发性的贡献

模型训练的好坏和数据的质量有很大的关系。像ImageNet这样的高质量数据集的搜集过程费时费力，想要进一步搜集成更大量的数据集成本过高。CLIP这篇论文提出可以直接从原始文本中学习图像。使用网络数据对，将文本作为图像标签进行训练，从而可以直接使用从网络上爬取的图片和其对应的文本，凑成网络数据对。通过实验证明使用网络数据对这样的形式来训练模型是有很好的效果的。借助这一方法，CLIP也成功将数据集的规模扩大到了4亿，可以大大地提升泛化性能。
# 模型结构
- 使用两个encoder分别处理文本和图片数据。text encoder使用transformer, image encoder使用ViT和ResNet
- encoder representation直接线性投影到multi-modal embedding space
- 计算2模态之间的cosine similarity，让N个匹配的图文对相似度最大，不匹配的图文对相似度最小
- 对称的cross-entropy loss
# 一些疑问
1.CLIP在ImageNet zero-shot上得到了与原始ResNet-50相似的准确性，而不需要使用它所训练的128万个训练示例中的任何一个。但是有没有可能4亿的训练数据集中包含了ImageNet的部分图片呢？不得而知 

2.