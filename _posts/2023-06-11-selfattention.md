---
layout:       post
title:        "Self Attention"
author:       "Ke"
header-style: text
catalog:      true
mathjax: true
tags:
    - Transformer
---

# Attention is All You Need 


Self-attention 就本质上是依然是一种特殊的 Attention。是种应用在 Transformer 中最重要的结构之一。  

 Attention机制能够帮我们找到子序列和全局的相关度的关系，也就是找到权重值$w_i$.Self-attention 对于 Attention 的变化，其实就是寻找权重值 $w_i$的过程不同。原来我们计算时使用的是子序列和全局，而现在我们计算 Self-attention 时，用的是自己和自己，这是 Attention 和 Self-attention 从计算上来说最大的区别。

 ## Self-attention的运算过程
 为了能够产生输出的向量 $y_i$，Self-attention 其实是对所有的输入做了一个加权平均的操作，这个公式和 Attention 是一致的  
 $$ y_i=\sum_j w_{ij} x_j$$
 $j$代表整个序列的长度，并且 $j$个权重的相加之和等于 1。值得一提的是，这里的$w_{ij}$
 并不是一个需要神经网络学习的参数，它是来源于$x_i$ 和 $x_j$（这里 $x_i$ 和 $x_j$就都是自己 self）的之间的计算的结果。而它们之间最简单的一种计算方式，就是使用点积的方式。
 $$ w_{ij}^{'}=x_i^{T}x_j.$$
 这个点积的输出的取值范围在负无穷和正无穷之间，所以我们要使用一个 Softmax 把它映射到$[0,1]$之间，并且要确保它们对于整个序列而言的和为 1.  
 $$w_{ij}=\frac{exp w_{ij}^{'}}{\sum_j exp w_{ij}^{'}}$$

 ## Attention 和 Self-attention 的区别是什么?

- 在神经网络中，通常来说你会有输入层（Input），应用激活函数后的输出层（Output），在 RNN 当中你会有状态（State）。如果 Attention (AT) 被应用在某一层的话，它更多的是被应用在输出或者是状态层上，而当我们使用 Self-attention（SA），这种注意力的机制更多的是在关注 Input 自己身上。
- SA 可以在一个模型当中被多次的、独立的使用（比如说在Transformer中，使用了18次；在Bert当中使用12次）。但是，AT在一个模型当中经常只是被使用一次，并且起到连接两个组件的作用。两个不同的组件（Component），编码器和解码器。但是如果我们用 SA，它就不是关注的两个组件，它只是在关注你应用的那一个组件。那这里他就不会去关注解码器了，就比如说在 Bert 中，使用的情况，我们就没有解码器。
- SA 可以在一个模型当中被多次的、独立的使用（比如说在 Transformer 中，使用了18次；在Bert当中使用 12次）。但是，AT 在一个模型当中经常只是被使用一次，并且起到连接两个组件的作用。
- SA 比较擅长在一个序列当中寻找不同部分之间的关系。比如说，在词法分析的过程中，SA能够帮助去理解不同词之间的关系。AT 却更擅长寻找两个序列之间的关系，比如说在翻译任务当中，原始的文本和翻译后的文本。这里也要注意，在翻译任务中，SA 也很擅长，比如Transformer。
- AT 可以连接两种不同的模态，比如说图片和文字。SA 更多的是被应用在同一种模态上，但是如果一定要使用SA来做的话，也可以将不同的模态组合成一个序列，再使用 SA。 

大部分情况，SA 这种结构更加的 general，在很多任务作为降维、特征表示、特征交叉等功能尝试着应用，很多时候效果都不错。

## Self-attention 为什么能 work?
考虑一个电影推荐系统，你要推荐给用户们一些他们更喜欢的电影，以便用户能够获得更好的体验。
一个可行的方法就是手动获取你电影中的特征，比如说这部电影关于爱情的部分有多少，关于动作的部分有多少；然后我们再去对用户的特征进行分析，比如说用户 A 对于爱情电影的喜爱程度有多少，对动作电影的喜爱程度有多少。如果我们按照上述方式构建了用户和电影的两个矩阵，那么它们的点积就会给你一个分数，这个分数就代表了用户对于某种电影的喜爱程度。
![img](/img/in-post/post-self-attention/score.jpg)
通过上面的这种计算方式，我们就能够得到一些 Score 值。这些值有正数也有负数。比如说，电影是一部关于爱情的电影，并且用户也很喜欢爱情电影，那么这个分值就是一个正数；如果电影是关于爱情的，但是用户却不喜欢爱情电影，那么这个分值就会是一个负值。

还有，这个分值的大小也表示了在某个属性上，它的程度是多大。比如说某一部电影，可能它的内容中只有一点点是关于爱情的，那么它的这个值就会很小；或者说有个用户他不是很喜欢爱情电影，那么这个值的绝对值就会很大，说明他对这种电影的偏见是很大的。

显而易见，上面说的这种方法在现实中是很难实现的。我们很难去人工标注上千万的电影的特征，和用户喜欢哪种类型的电影的分值。因而我们制作模型的电影特征和用户特征参数。然后，我们向用户收集他们喜欢的电影，并优化用户特征和电影特征，使他们的点积与已知用户的喜欢相匹配。尽管我们没有告诉模型任何特征应该是什么意思，但在实践中证明，在训练后，这些特征实际上反映了关于电影内容的有意义的语义。

![img](/img/in-post/post-self-attention/basic-self-attn.jpg)

上图是自注意力机制实现的一个方法概览。


这是应用Self-attention的原则。假设我们面对的是一系列的单词。为了应用Self-attention，我们只需为词汇表中的每个单词$t$分配一个$Embedding$向量$v_t$（我们将学习其$values$）。这就是序列建模中的$Embedding\ layer$。它将单词序列$the,cat,walks,on,the,street$转换为向量序列  

$$v_{the},v_{cat},v_{walks},v_{on},v_{the},v_{street}$$

如果我们将这个序列喂入自注意力层，那么我们得到另一个输出序列
$$y_{the},y_{cat},y_{walks},y_{on},y_{the},y_{street}$$
那么$y_{cat}$就是所有在第一个序列中的 Embedding 向量的加权和，权重值就是标准化后的Embedding与$v_{cat}$之间的点积  
上文中我们也提到了，$v_t$ 
 是我们学习到的 Embedding 向量，它是$t$ 这个单词向量化的表示。在大部分的场景中，$the$这个单词和句子中的其他单词没有很强的相关性，因此，我们就会期待 $v_{the}$
 和其他单词的点积结果应该比较小或者是一个负值。那再看$walks$
 这个单词，为了能够解释这个单词，我们希望能够知道是谁在 $walk$，在这句话当中,$v_{cat}$
 和$v_{walks}$ 的点积就应该有一个比较大的正的值.

 - 到目前为止，我们还没有用到需要学习的参数。基础的 Self-attention 实际上完全取决于我们创建的输入序列，上游的 Embeding Layer 驱动着 Self-attention 学习对于文本语义的向量表示。
- Self-attention看到的序列只是一个集合(set)，不是一个序列，它并没有顺序。如果我们重新排列集合，输出的序列也是一样的。后面我们要使用一些方法来缓和这种没有顺序所带来的信息的缺失。但是值得一提的是，Self-attention 本身是忽略序列的自然输入顺序的。



