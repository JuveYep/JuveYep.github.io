---
layout:       post
title:        "Attention"
author:       "Ke"
header-style: text
catalog:      true
mathjax: true
tags:
    - Transformer
---
>Attention
# Attention机制的用处,[[code]](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)

$Attention$[[paper]]([[paper]](https://arxiv.org/pdf/1409.0473.pdf)),由 Bengio 于2015提出。

借鉴了生物在观察、学习、思考行为中的过程的一种独特的生理机制，这种机制就是 Attention 机制。大家都能有感觉，我们在获取信息的时候，通常是先从宏观上建立一个比较模糊的认识，然后又在宏观认识下，发现一些比较重要的信息，对于这些重要的信息，我们花费更多的注意力进行观察、学习和思考。

$Attention$可以用于**计算机视觉，自然语言处理，推荐系统**中

## 1.$CV$
在视觉任务中，$Attention$的直观理解就是我们观察图片时聚焦的内容，聚焦于不同的地方我们所得到的信息就会有很多的差别。
![img](/img/in-post/post-attention/cv1.jpg)
比如这张图片，我们能看到汉堡王，大巴，街道，行人等等。当我们描述这些信息的时候，我们的$Attention$就聚焦在不同的地方。

进一步的我们要是想知道更多的信息，比如说这张图片是什么时间段拍的，早晨还是傍晚？这就需要我们的$Attention$集中在有关时间段的信息上，比如说天空，店铺的灯光这些都可以为我们提供相应的信息。

在我的理解看来这也就是$Local\ Representations$这一过程。

## 2.$NLP$
在$NLP$中$Attention$机制的作用是什么呢？

举个例子，She is eating a <span style="color:green;">green</span> <span style="color:red;">apple</span>这句话中，我们可以看出谓语$eating$和宾语$apple$，主语$She$之间的关系很强，而谓语$eating$和形容词$green$之间的关系不是那么强，也就是说，我们只需要关注主语$She$，谓语$eating$，宾语$apple$，那么我们就可以粗略地判断出来这个句子所要表达的意思，能够帮助我们更好的理解“吃”这个动作。

$Attention$的机制能够帮助我们在处理单个的 $token$ 的时候，带有一定的上下文信息。这就像是一种“软性记忆”一样，帮助我们记住上下文中包含的信息。

当我们看一篇文章的时候，其实也是类似的。我们从拿到一篇文章开始，首先关注的也只是一些关键性的词语，这些关键性的词语，就能够帮助我们快速的判断文章的内容和结构。这些场景就是我们在一些具体场景中对$Attention$的应用。

## 3.推荐系统
通常来说，我们构建的推荐系统是通过对于用户历史行为、用户特征、物品特征等的观测，来判断用户是否会对一个新的物品感兴趣。
![img](/img/in-post/post-attention/recommend.jpg)
比如在上图中，从左到右表示用户从过去到现在所购买的商品序列，现在就是要通过观察这些序列、用户特征、商品特征，来判断新出现的一个商品，用户会不会购买。

再这样的一个判断的过程中，我们会发现，购买序列中的某一些商品对于当前的这个商品购买是具有局定性的指导作用的，而其他的商品可能就没那么的重要。比如说，我购买了手套、靴子，就对我可能会购买羽绒服的决定比我买手机、键盘要更重要一些。这个重要性，就是我们希望能够从购买序列中发现的，他能帮着我们更好的判断新商品购买的可能性。

这个重要性其实就是注意力！我们的模型在去做判断的时候，到底应该把哪些物品当成是重要的判断依据，这能大大的提高我们模型的准确性。

## 4.知识图谱

通常由实体和它们之间的关系构成。以下是几个示例：

### 实体链接（Entity Linking）
在知识图谱中，实体链接是将文本中的实体链接到知识图谱中相应的实体的过程。使用Attention机制可以帮助确定文本中的特定实体与知识图谱中的哪些实体最相关。

### 实体对齐（Entity Alignment）
在多个知识图谱之间，实体对齐是将相同或相似的实体对应起来的任务。Attention机制可以用于计算实体之间的相似度或匹配度，从而辅助实体对齐任务。

### 关系抽取（Relation Extraction）
知识图谱中的关系表示实体之间的语义关联。Attention机制可以在关系抽取任务中帮助模型关注实体对之间的关键信息，以便准确地识别和分类它们之间的关系。

### 图神经网络（Graph Neural Networks）
图神经网络是一类用于处理图数据的机器学习模型。Attention机制可以在图神经网络中用于学习节点之间的关联权重，以更好地捕捉节点之间的局部和全局信息。

### 图生成（Graph Generation）
利用Attention机制，可以根据已有的知识图谱生成新的图结构。通过注意力权重，模型可以选择性地关注特定实体和关系，以生成具有一定连贯性和相关性的新图。

这些仅仅是Attention在知识图谱中的一些应用示例。实际应用中，Attention机制可以用于各种任务，以提高模型对知识图谱的理解和应用能力。

## 4.$Attention$的工作原理
