---
layout:       post
title:        "Attention"
author:       "Ke"
header-style: text
catalog:      true
mathjax: true
tags:
    - Transformer
---
# $Attention$

$Attention$[[paper]]([[paper]](https://arxiv.org/pdf/1409.0473.pdf)),由 Bengio 于2015提出。

借鉴了生物在观察、学习、思考行为中的过程的一种独特的生理机制，这种机制就是 Attention 机制。大家都能有感觉，我们在获取信息的时候，通常是先从宏观上建立一个比较模糊的认识，然后又在宏观认识下，发现一些比较重要的信息，对于这些重要的信息，我们花费更多的注意力进行观察、学习和思考。

$Attention$可以用于**计算机视觉，自然语言处理，推荐系统**中

## 1.$CV$
在视觉任务中，$Attention$的直观理解就是我们观察图片时聚焦的内容，聚焦于不同的地方我们所得到的信息就会有很多的差别。
![img](/img/in-post/post-attention/cv1.jpg)
比如这张图片，我们能看到汉堡王，大巴，街道，行人等等。当我们描述这些信息的时候，我们的$Attention$就聚焦在不同的地方。

进一步的我们要是想知道更多的信息，比如说这张图片是什么时间段拍的，早晨还是傍晚？这就需要我们的$Attention$集中在有关时间段的信息上，比如说天空，店铺的灯光这些都可以为我们提供相应的信息。

在我的理解看来这也就是$Local\ Representations$这一过程。

## 2.$NLP$
在$NLP$中$Attention$机制的作用是什么呢？

举个例子，She is eating a <span style="color:green;">green</span> <span style="color:red;">apple</span>这句话中，我们可以看出谓语$eating$和宾语$apple$，主语$She$之间的关系很强，而谓语$eating$和形容词$green$之间的关系不是那么强，也就是说，我们只需要关注主语$She$，谓语$eating$，宾语$apple$，那么我们就可以粗略地判断出来这个句子所要表达的意思，能够帮助我们更好的理解“吃”这个动作。

$Attention$的机制能够帮助我们在处理单个的 $token$ 的时候，带有一定的上下文信息。这就像是一种“软性记忆”一样，帮助我们记住上下文中包含的信息。

当我们看一篇文章的时候，其实也是类似的。我们从拿到一篇文章开始，首先关注的也只是一些关键性的词语，这些关键性的词语，就能够帮助我们快速的判断文章的内容和结构。这些场景就是我们在一些具体场景中对$Attention$的应用。

## 3.推荐系统
通常来说，我们构建的推荐系统是通过对于用户历史行为、用户特征、物品特征等的观测，来判断用户是否会对一个新的物品感兴趣。
![img](/img/in-post/post-attention/recommend.jpg)
比如在上图中，从左到右表示用户从过去到现在所购买的商品序列，现在就是要通过观察这些序列、用户特征、商品特征，来判断新出现的一个商品，用户会不会购买。

再这样的一个判断的过程中，我们会发现，购买序列中的某一些商品对于当前的这个商品购买是具有局定性的指导作用的，而其他的商品可能就没那么的重要。比如说，我购买了手套、靴子，就对我可能会购买羽绒服的决定比我买手机、键盘要更重要一些。这个重要性，就是我们希望能够从购买序列中发现的，他能帮着我们更好的判断新商品购买的可能性。

这个重要性其实就是注意力！我们的模型在去做判断的时候，到底应该把哪些物品当成是重要的判断依据，这能大大的提高我们模型的准确性。

## 4.知识图谱

通常由实体和它们之间的关系构成。以下是几个示例：

### 实体链接（Entity Linking）
在知识图谱中，实体链接是将文本中的实体链接到知识图谱中相应的实体的过程。使用Attention机制可以帮助确定文本中的特定实体与知识图谱中的哪些实体最相关。从而可以计算实体之间的相似度或匹配度，从而辅助实体对齐任务。

### 关系抽取（Relation Extraction）
知识图谱中的关系表示实体之间的语义关联。Attention机制可以在关系抽取任务中帮助模型关注实体对之间的关键信息，以便准确地识别和分类它们之间的关系。

### 图神经网络（Graph Neural Networks）
图神经网络是一类用于处理图数据的机器学习模型。Attention机制可以在图神经网络中用于学习节点之间的关联权重，以更好地捕捉节点之间的局部和全局信息。可以根据已有的知识图谱生成新的图结构。通过注意力权重，模型可以选择性地关注特定实体和关系，以生成具有一定连贯性和相关性的新图。

这些仅仅是Attention在知识图谱中的一些应用示例。实际应用中，Attention机制可以用于各种任务，以提高模型对知识图谱的理解和应用能力。

## 4.$Attention$的工作原理

一个 $Attention$ 的计算过程有三步：  
1.$query$ 和 $key$ 进行相似度计算，得到一个 $query$ 和 $key$ 相关性的分值；  
2.将这个分值进行归一化($softmax$)，得到一个注意力的分布；  
3.使用注意力分布和 $value$ 进行计算，得到一个融合注意力的更好的 $value$ 值。   
比如这张图
![img](/img/in-post/post-attention/encode-decode.jpg)
在没有注意力之前，我们每次都是根据 $Encoder$ 部分的输出结果来进行生成，提出注意力后，就是想在生成翻译结果时并不是看 $Encoder$ 中所有的输出结果，而是先来看看，我想生成的这部分和哪些单词可能关系会比较大，关系大的我多借鉴些；关系小的，少借鉴些。就是这样一个想法，我们看看该如何操作。

- 我们把 $Decoder$ 部分输入后得到的向量作为 $query$；把 $Encoder$ 部分每个单词的向量作为 $key$。我们先把 $query$ 和每一个单词进行点乘 ，得到相关性的分值；
 
- 有了这些分值后，我们对这些分值做一个 $softmax$
 ，得到一个注意力的分布

- 有了这个注意力，我们就可以用它和 $Encoder$ 的输出值 ($value$) 进行相乘，得到一个加权求和后的值，这个值就包含注意力的表示，我们用它来预测要生成的词

<video width="640" height="360" controls>
  <source src="/img/in-post/post-attention/encode-decode.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## 5.在推荐系统中的应用

在商品推荐系统中，$query$ 就是我当前要判断的商品的向量，$key$ 就是用户购买序列中，每一个商品的向量。

- $query$ 和 $key$ 进行相似度计算，得到待判断商品和购买序列中商品的相关性分值；
- 将这个分值进行归一化($softmax$)，得到一个商品注意力的分布，看看哪些商品是判断的重要依据；
- 使用注意力分布和 $value$ 进行计算，得到一个融合注意力的更好的 $value$ 值，这个值就是最终我们融合判断当前商品是否推荐购买的依据。  

当然，$Attention$ 并不是只有这一种计算方式，后来还有很多人找到了各种各样的计算注意力的方法。但是从本质上，它们都遵循着这个三步走的逻辑，如果你能把对它的理解，放到你的任务中，那你就离着创造自己的 $Attention$ 不远了。

总的而言，$Attention$的三步走如下：  
$query$ 和 $key$ 进行相似度计算，得到一个 $query$ 和 $key$ 相关性的分值；
将这个分值进行归一化($softmax$)，得到一个注意力的分布；
使用注意力分布和 $value$ 进行计算，得到一个融合注意力的更好的 $value$ 值。

### 6.由此想到在知识图谱反欺诈中的应用
$query$ 就是我当前要判断的用户收到的信息的语义向量，$key$ 就是用户收到信息序列中每一条包含欺诈信息的向量。
- $query$ 和 $key$ 进行相似度计算，得到待判断语义向量和包含欺诈信息的向量的相关性分值；
- 将这个分值进行归一化($softmax$)，得到一个语义向量注意力的分布，看看哪些语义是判断的重要依据；
- 使用注意力分布和 $value$ 进行计算，得到一个融合注意力的更好的 $value$ 值，这个值就是最终我们融合判断当前语义是否包含欺诈信息的依据。  